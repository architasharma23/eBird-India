[["index.html", "Source code and supplementary material for independent validation of State of India’s birds Section 1 Introduction 1.1 Data processing", " Source code and supplementary material for independent validation of State of India’s birds Vijay Ramesh 2022-10-06 Section 1 Introduction This is the readable version containing analysis that examines eBird data (specifics of the analysis will be updated). Methods and format are derived from https://cornelllabofornithology.github.io/ebird-best-practices/. 1.1 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],["spatial-sampling-no-filters.html", "Section 2 Spatial sampling (no filters) 2.1 Prepare libraries 2.2 Subset observations by geographical confines of the study area 2.3 Data cleaning prior to visualization 2.4 Load spatial grids for checklist locations 2.5 Get proportional observation counts across every 25 km x 25 km grid 2.6 Filter data only for a specific list of species 2.7 Checklist distribution 2.8 Figure: Checklist distribution 2.9 What species are reported sufficiently in checklists? 2.10 Figure: Proportion of checklists reporting species in each grid cell 2.11 write the species lists to file 2.12 save .RData for future scripts", " Section 2 Spatial sampling (no filters) In this script, we will examine data at the species level for a chosen list of species, across its entire geographic range in the Indian subcontinent. Our aim here is to first understand the proportion of checklists that report the species of interests for every 25 x 25 km grid size (finer scale will be chosen for comparison in a future script). No other filters are applied in this particular script. 2.1 Prepare libraries # load libraries library(data.table) library(readxl) library(magrittr) library(stringr) library(dplyr) library(tidyr) library(readr) library(ggplot2) library(ggthemes) library(scico) library(extrafont) library(sf) library(rnaturalearth) # round any function round_25 &lt;- function(x, accuracy = 25000) { round(x / accuracy) * accuracy } # set file paths for auk functions # To use these two datasets, please download the latest versions from https://ebird.org/data/download and set the file path accordingly. Since these two datasets are extremely large, we have not uploaded the same to GitHub. # In this study, the version of data loaded corresponds to July 2022. f_in_ebd &lt;- file.path(&quot;data/ebd_IN_relJul-2022.txt&quot;) f_in_sampling &lt;- file.path(&quot;data/ebd_sampling_relJul-2022.txt&quot;) 2.2 Subset observations by geographical confines of the study area In this case, the geographical confines correspond to the political boundary of India. # first, we load the India states boundary from GADM india_states &lt;- st_read(&quot;data/spatial/shapefiles/india_states_boundary.shp&quot;) box_in &lt;- st_bbox(india_states) # read in ebd data and subset by state for our analysis # To access the latest dataset, please visit: https://ebird.org/data/download and set the file path accordingly. ebd &lt;- fread(&quot;data/ebd_IN_relJul-2022.txt&quot;) # subsettting by lat-long ebd_in &lt;- ebd[between(LONGITUDE, box_in[&quot;xmin&quot;], box_in[&quot;xmax&quot;]) &amp; between(LATITUDE, box_in[&quot;ymin&quot;], box_in[&quot;ymax&quot;]), ] # resulted in removal of 21 observations in total # make new column names newNames &lt;- str_replace_all(colnames(ebd_in), &quot; &quot;, &quot;_&quot;) %&gt;% str_to_lower() setnames(ebd_in, newNames) # keep useful columns columnsOfInterest &lt;- c( &quot;common_name&quot;, &quot;scientific_name&quot;, &quot;observation_count&quot;, &quot;locality&quot;, &quot;state&quot;, &quot;locality_id&quot;, &quot;locality_type&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;observation_date&quot;, &quot;time_observations_started&quot;, &quot;protocol_type&quot;,&quot;duration_minutes&quot;,&quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;group_identifier&quot;, &quot;reviewed&quot;, &quot;sampling_event_identifier&quot; ) ebd_in &lt;- ebd_in[, ..columnsOfInterest] # remove the large ebd file if it&#39;s not required rm(ebd) gc() # run this function to clear up some memory space 2.3 Data cleaning prior to visualization # Convert all presences marked &#39;X&#39; as &#39;1&#39; ebd_in &lt;- ebd_in %&gt;% mutate(observation_count = ifelse(observation_count == &quot;X&quot;, &quot;1&quot;, observation_count )) # Convert observation count to numeric ebd_in$observation_count &lt;- as.numeric(ebd_in$observation_count) 2.4 Load spatial grids for checklist locations Add a spatial filter. # strict spatial filter and assign grid locs &lt;- ebd_in[, .(longitude, latitude)] # transform to UTM coords &lt;- setDF(locs) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% bind_cols(as.data.table(st_coordinates(.))) %&gt;% st_transform(&quot;+proj=laea +lon_0=80.859375 +lat_0=20.6486698 +datum=WGS84 +units=m +no_defs&quot;) %&gt;% mutate(id = 1:nrow(.)) # make some empty space gc() # convert to UTM for filter india_states &lt;- st_transform(india_states, &quot;+proj=laea +lon_0=80.859375 +lat_0=20.6486698 +datum=WGS84 +units=m +no_defs&quot;) coords &lt;- coords %&gt;% filter(id %in% unlist(st_contains(india_states, coords))) %&gt;% rename(longitude = X, latitude = Y) %&gt;% bind_cols(as.data.table(st_coordinates(.))) %&gt;% st_drop_geometry() %&gt;% as.data.table() # remove unneeded objects rm(locs) gc() coords &lt;- coords[, .N, by = .(longitude, latitude, X, Y)] ebd_in &lt;- merge(ebd_in, coords, all = FALSE, by = c(&quot;longitude&quot;, &quot;latitude&quot;)) ebd_in &lt;- ebd_in[(longitude %in% coords$longitude) &amp; (latitude %in% coords$latitude), ] 2.5 Get proportional observation counts across every 25 km x 25 km grid Analysis of proportional counts estimated at 25km ## Analysis of proportional counts at 25 km for each state # round to 25km cell in UTM coords ebd_in[, `:=`(X = round_25(X), Y = round_25(Y))] # count checklists in cell ebd_summary &lt;- ebd_in[, nchk := length(unique(sampling_event_identifier)), by = .(X, Y)] # count checklists reporting each species in cell and get proportion ebd_summary &lt;- ebd_summary[, .(nrep = length(unique( sampling_event_identifier ))), by = .(X, Y, nchk, scientific_name) ] ebd_summary[, p_rep := nrep / nchk] # complete the dataframe for no reports # keep no reports as NA --- allows filtering based on proportion reporting ebd_summary &lt;- setDF(ebd_summary) %&gt;% complete( nesting(X, Y), scientific_name # , # fill = list(p_rep = 0) ) %&gt;% filter(!is.na(p_rep)) 2.6 Filter data only for a specific list of species We filtered data for a subset of 891 species. # remove species that are poorly reported (only filter being applied at this stage) # for example, accounts such as &#39;Turdidae sp.&#39;, are removed and only the latest # list of scientific names are taken into consideration species_list &lt;- read.csv(&quot;data/2022-SoIB-species-list.csv&quot;) speciesOfInterest &lt;- species_list %&gt;% filter(Selected...SOIB ==&quot;X&quot;) speciesOfInterest &lt;- data.frame(speciesOfInterest$Scientific.Name) names(speciesOfInterest) &lt;- &quot;scientific_name&quot; # filter for species list above - we are choosing only the 891 species ebd_summary &lt;- ebd_summary[ebd_summary$scientific_name %in% speciesOfInterest$scientific_name, ] 2.7 Checklist distribution # total number of checklists across unique grids tot_n_chklist &lt;- ebd_summary %&gt;% distinct(X, Y, nchk) # India has a total of 4792 unique 25 x 25 km grids # species-specific number of grids spp_grids &lt;- ebd_summary %&gt;% group_by(scientific_name) %&gt;% distinct(X, Y) %&gt;% count(scientific_name, name = &quot;n_grids&quot; ) # Write the above two results write.csv(spp_grids, &quot;results/01_ngrids-per-spp.csv&quot;, row.names=F) # left-join the datasets ebd_summary &lt;- left_join(ebd_summary, spp_grids, by = &quot;scientific_name&quot;) 2.8 Figure: Checklist distribution # add land land &lt;- ne_countries( scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;) ) # crop land land &lt;- st_transform(land, &quot;+proj=laea +lon_0=80.859375 +lat_0=20.6486698 +datum=WGS84 +units=m +no_defs&quot;) Number of checklists over time 2.9 What species are reported sufficiently in checklists? Similar to Ramesh et al. (2022), we visualized and listed species that occurred in atleast 5% of all checklists across half of the unique study area grids they were reported from. For example, let us consider the case of Accipiter nisus. This species has been reported from 1000 unique (25 x 25 km grids) across India. There are 206 unique grids in which this species has been reported in over 5% of all checklists from that grid. p_cutoff &lt;- 0.05 # Proportion of checklists a species has been reported in grid_proportions &lt;- ebd_summary %&gt;% group_by(scientific_name) %&gt;% tally(p_rep &gt;= p_cutoff) %&gt;% mutate(prop_grids_cut = n / (spp_grids$n_grids)) %&gt;% arrange(desc(prop_grids_cut)) grid_prop_cut &lt;- filter( grid_proportions, prop_grids_cut &gt;= p_cutoff ) # Write the results write.csv(grid_prop_cut, &quot;results/01_prop-grids-per-spp.csv&quot;, row.names = F) # Identifying the number of species that occur in potentially &lt;5% of all lists total_number_lists &lt;- sum(tot_n_chklist$nchk) spp_sum_chk &lt;- ebd_summary %&gt;% distinct(X, Y, scientific_name, nrep) %&gt;% group_by(scientific_name) %&gt;% mutate(sum_chk = sum(nrep)) %&gt;% distinct(scientific_name, sum_chk) # 88 species of the list of 877 occur in &gt;5% of all checklists prop_all_lists &lt;- spp_sum_chk %&gt;% mutate(prop_lists = sum_chk / total_number_lists) %&gt;% arrange(desc(prop_lists)) 2.10 Figure: Proportion of checklists reporting species in each grid cell Note: Here we plot all species that occur in atleast 5% of all checklists across half of the 25 x 25 km cells from where they have been reported 2.11 write the species lists to file # write the list of species that occur in at least 5% of checklists across a minimum of 50% of the grids they have been reported in sp_list &lt;- semi_join(speciesOfInterest, grid_prop_cut, by = &quot;scientific_name&quot;) write.csv(sp_list, &quot;results/01_list-of-species-cutoff50.csv&quot;, row.names = F) 2.12 save .RData for future scripts save.image(file = &quot;data/01_ebd-India.RData&quot;) "],["temporal-sampling.html", "Section 3 Temporal sampling 3.1 Prepare libraries 3.2 Filter data using auk package 3.3 Process filtered data 3.4 Include only species with scientific names 3.5 Subset data by time period 3.6 Subset data spatially", " Section 3 Temporal sampling In this script, we examine temporal differences in the reporting of observations for the states of Tamil Nadu and Kerala. The analyses we carry out include: a) reporting of checklists for every 5-year interval b) are there significant differences between pre2000 and other time periods 3.1 Prepare libraries # load libraries library(data.table) library(readxl) library(magrittr) library(stringr) library(dplyr) library(tidyr) library(readr) library(ggplot2) library(ggthemes) library(scico) library(extrafont) library(sf) library(rnaturalearth) library(auk) # round any function round_25 &lt;- function(x, accuracy = 25000) { round(x / accuracy) * accuracy } # set file paths for auk functions # To use these two datasets, please download the latest versions from https://ebird.org/data/download and set the file path accordingly. Since these two datasets are extremely large, we have not uploaded the same to GitHub. # In this study, the version of data loaded corresponds to July 2022. f_in_ebd &lt;- file.path(&quot;data/ebd_IN_relJul-2022.txt&quot;) f_in_sampling &lt;- file.path(&quot;data/ebd_sampling_relJul-2022.txt&quot;) 3.2 Filter data using auk package Here, we set broad spatial filters to subset all complete checklsits for the states of Kerala and Tamil Nadu for the above list of species. # run filters using auk packages ebd_filters &lt;- auk_ebd(f_in_ebd, f_in_sampling) %&gt;% auk_country(country = &quot;IN&quot;) %&gt;% auk_state(c(&quot;IN-KL&quot;, &quot;IN-TN&quot;)) %&gt;% auk_complete() # check filters ebd_filters Below code need not be run if it has been filtered once already and the above path leads to the right dataset. NB: This is a computation heavy process, run with caution. # specify output location and perform filter f_out_ebd &lt;- &quot;data/01_ebird-filtered-KL-TN.txt&quot; f_out_sampling &lt;- &quot;data/01_ebird-filtered-sampling-KL-TN.txt&quot; ebd_filtered &lt;- auk_filter(ebd_filters, file = f_out_ebd, file_sampling = f_out_sampling, overwrite = TRUE ) 3.3 Process filtered data The data has been filtered above using the auk functions. We will now work with the filtered checklist observations. # read in the data # note: running this takes a very long time and I would suggest saving an .RData file after applying the basic filters ebd &lt;- read_ebd(f_out_ebd) gc() # Convert all presences marked &#39;X&#39; as &#39;1&#39; ebd &lt;- ebd %&gt;% mutate(observation_count = ifelse(observation_count == &quot;X&quot;, &quot;1&quot;, observation_count)) # Convert observation count to numeric ebd$observation_count &lt;- as.numeric(ebd$observation_count) # choose columns of interest columnsOfInterest &lt;- c( &quot;checklist_id&quot;, &quot;scientific_name&quot;, &quot;common_name&quot;, &quot;observation_count&quot;, &quot;locality&quot;, &quot;locality_id&quot;, &quot;locality_type&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;observation_date&quot;, &quot;time_observations_started&quot;, &quot;observer_id&quot;, &quot;sampling_event_identifier&quot;, &quot;protocol_type&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;effort_area_ha&quot;, &quot;number_observers&quot;, &quot;species_observed&quot;,&quot;state&quot;, &quot;reviewed&quot; ) ebd &lt;- ebd %&gt;% select(matches(columnsOfInterest)) 3.4 Include only species with scientific names # remove species that are poorly reported (only filter being applied at this stage) # for example, accounts such as &#39;Turdidae sp.&#39;, are removed and only the latest # list of scientific names are taken into consideration trait_dat &lt;- read.csv(&quot;data/2022-SoIB-habitat-data.csv&quot;) speciesOfInterest &lt;- data.frame(trait_dat$eBird.Scientific.Name.2021) names(speciesOfInterest) &lt;- &quot;scientific_name&quot; # filter for species list above ebd &lt;- ebd[ebd$scientific_name %in% speciesOfInterest$scientific_name, ] 3.5 Subset data by time period We will divide data into five-year intervals: 2000-2005, 2006-2010, 2011-2015, 2016-2020 and all data before 2000 as a single category. # separate observation_date as year, month, day ebd &lt;- ebd %&gt;% separate(observation_date, c(&quot;year&quot;,&quot;month&quot;,&quot;day&quot;)) # subset data by time period ebd &lt;- ebd %&gt;% mutate(timePeriod = case_when( year &gt;= 2000 &amp; year &lt; 2005 ~ &quot;2000-2005&quot;, year &gt;= 2005 &amp; year &lt; 2010 ~ &quot;2005-2010&quot;, year &gt;= 2010 &amp; year &lt; 2015 ~ &quot;2010-2015&quot;, year &gt;= 2015 &amp; year &lt; 2020 ~ &quot;2015-2020&quot;, year &gt;= 2020 ~ &quot;post2020&quot;, year &lt; 2000 ~ &quot;pre2000&quot; )) 3.6 Subset data spatially # read in shapefile of the study area to subset by bounding box # first, we load the India states boundary from GADM india_states &lt;- st_read(&quot;data/spatial/shapefiles/IND_adm1.shp&quot;) # load tamil nadu and kerala separately tamil_nadu &lt;- india_states[india_states$NAME_1==&quot;Tamil Nadu&quot;,] box_tn &lt;- st_bbox(tamil_nadu) kerala &lt;- india_states[india_states$NAME_1==&quot;Kerala&quot;,] box_ker &lt;- st_bbox(kerala) # get data spatial coordinates dataLocs &lt;- data %&gt;% map(function(x) { select(x, longitude, latitude) %&gt;% filter(between(longitude, box[&quot;xmin&quot;], box[&quot;xmax&quot;]) &amp; between(latitude, box[&quot;ymin&quot;], box[&quot;ymax&quot;])) }) %&gt;% bind_rows() %&gt;% distinct() %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% st_set_crs(4326) %&gt;% st_intersection(hills) tn_coords &lt;- setDF(tn_locs) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% bind_cols(as.data.table(st_coordinates(.))) %&gt;% st_transform(32643) %&gt;% mutate(id = 1:nrow(.)) # tamil nadu tn_ebd &lt;- ebd %&gt;% filter(between(longitude, box_tn[&quot;xmin&quot;], box_tn[&quot;xmax&quot;]) &amp; between(latitude, box_tn[&quot;ymin&quot;], box_tn[&quot;ymax&quot;])) # tamil nadu ker_ebd &lt;- ebd %&gt;% filter(between(longitude, box_ker[&quot;xmin&quot;], box_ker[&quot;xmax&quot;]) &amp; between(latitude, box_ker[&quot;ymin&quot;], box_ker[&quot;ymax&quot;])) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
